{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "carol-rnn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvydtHQBW0Za",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = open(\"/content/carol.txt\",\"r\")\n",
        "carol = data.read().lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTk8RxfyZDkE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "bc5aac66-a691-4616-9492-f602ef9cddaf"
      },
      "source": [
        "chars = sorted(list(set(carol)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars)) #중복제거 매핑\n",
        "n_chars = len(carol)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: \", n_chars)\n",
        "print (\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  942\n",
            "Total Vocab:  25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMpFhMpKZRiR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ab05d3e2-9ed4-4cac-95d7-41e24483a545"
      },
      "source": [
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "    seq_in = carol[i:i + seq_length]\n",
        "    seq_out = carol[i + seq_length]\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total Patterns: \", n_patterns) "
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  842\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cmhzFXuaDit",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import np_utils\n",
        "\n",
        "x = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "x = x / float(n_vocab)\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtIN_2KyWSPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6p0674UauK7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8c38ef88-8081-465c-952e-34e96ec2e8ce"
      },
      "source": [
        "hist = model.fit(X, y, nb_epoch=100, batch_size=128)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "842/842 [==============================] - 3s 4ms/step - loss: 3.1444\n",
            "Epoch 2/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.9787\n",
            "Epoch 3/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.9453\n",
            "Epoch 4/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.9275\n",
            "Epoch 5/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.9221\n",
            "Epoch 6/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.9171\n",
            "Epoch 7/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.9122\n",
            "Epoch 8/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.9171\n",
            "Epoch 9/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.9145\n",
            "Epoch 10/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.9147\n",
            "Epoch 11/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.9185\n",
            "Epoch 12/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.9163\n",
            "Epoch 13/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.9137\n",
            "Epoch 14/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.9045\n",
            "Epoch 15/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.9046\n",
            "Epoch 16/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.9103\n",
            "Epoch 17/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.9015\n",
            "Epoch 18/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.9068\n",
            "Epoch 19/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.8987\n",
            "Epoch 20/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.9037\n",
            "Epoch 21/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.9051\n",
            "Epoch 22/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.8970\n",
            "Epoch 23/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.8972\n",
            "Epoch 24/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.8795\n",
            "Epoch 25/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.8691\n",
            "Epoch 26/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.8416\n",
            "Epoch 27/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.8239\n",
            "Epoch 28/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.8004\n",
            "Epoch 29/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.7805\n",
            "Epoch 30/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.7565\n",
            "Epoch 31/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.7432\n",
            "Epoch 32/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.7144\n",
            "Epoch 33/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.6811\n",
            "Epoch 34/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.6423\n",
            "Epoch 35/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.6137\n",
            "Epoch 36/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.5749\n",
            "Epoch 37/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.5193\n",
            "Epoch 38/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.4683\n",
            "Epoch 39/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.5043\n",
            "Epoch 40/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.4222\n",
            "Epoch 41/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.3461\n",
            "Epoch 42/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.3210\n",
            "Epoch 43/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.2113\n",
            "Epoch 44/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.1445\n",
            "Epoch 45/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.0778\n",
            "Epoch 46/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 2.0169\n",
            "Epoch 47/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 1.8893\n",
            "Epoch 48/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 1.8004\n",
            "Epoch 49/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 1.7132\n",
            "Epoch 50/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 1.6069\n",
            "Epoch 51/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 1.5839\n",
            "Epoch 52/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 1.5126\n",
            "Epoch 53/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 1.5013\n",
            "Epoch 54/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 1.4137\n",
            "Epoch 55/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 1.2746\n",
            "Epoch 56/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 1.1699\n",
            "Epoch 57/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 1.0848\n",
            "Epoch 58/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 1.0260\n",
            "Epoch 59/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.9694\n",
            "Epoch 60/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.9283\n",
            "Epoch 61/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.8518\n",
            "Epoch 62/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.7955\n",
            "Epoch 63/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.7508\n",
            "Epoch 64/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.7097\n",
            "Epoch 65/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.6340\n",
            "Epoch 66/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.5810\n",
            "Epoch 67/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.5604\n",
            "Epoch 68/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.5246\n",
            "Epoch 69/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.4855\n",
            "Epoch 70/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.4462\n",
            "Epoch 71/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.4214\n",
            "Epoch 72/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.3998\n",
            "Epoch 73/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.4167\n",
            "Epoch 74/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.3820\n",
            "Epoch 75/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.3520\n",
            "Epoch 76/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.3326\n",
            "Epoch 77/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.3069\n",
            "Epoch 78/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.3081\n",
            "Epoch 79/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.2762\n",
            "Epoch 80/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.2908\n",
            "Epoch 81/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.2528\n",
            "Epoch 82/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.2582\n",
            "Epoch 83/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.2356\n",
            "Epoch 84/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.2187\n",
            "Epoch 85/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.2178\n",
            "Epoch 86/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.1976\n",
            "Epoch 87/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.1965\n",
            "Epoch 88/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.1855\n",
            "Epoch 89/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.1864\n",
            "Epoch 90/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.1736\n",
            "Epoch 91/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.1647\n",
            "Epoch 92/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.1675\n",
            "Epoch 93/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.1628\n",
            "Epoch 94/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.1577\n",
            "Epoch 95/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.1431\n",
            "Epoch 96/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.1534\n",
            "Epoch 97/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.1502\n",
            "Epoch 98/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.1401\n",
            "Epoch 99/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.1304\n",
            "Epoch 100/100\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.1281\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHZ2fKNka_Bc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "8393e595-9b03-453d-9e56-c5c36563ce2b"
      },
      "source": [
        "import sys\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(100):\n",
        "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = np.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "print (\"\\nDone.\")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" ristmas and a happy new year\n",
            "good tidings we bring to you and your kin\n",
            "we wish you a merry christmas \"\n",
            " and a happy new year\n",
            "we wis't go until we get some\n",
            "we won't go until we get some\n",
            "we won't go until \n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
